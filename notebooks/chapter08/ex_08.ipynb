{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f9ed1-0b83-426f-bc83-c0062bab8c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Ch08 - ex08\").getOrCreate()\n",
    "\n",
    "collection = [1, \"two\", 3.0, (\"four\", 4), {\"five\", 5}]\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)  # promote list to RDD\n",
    "\n",
    "print(collection_rdd)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123da6c8-0482-4ff7-ae40-dac59e5a989b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71c0ec-fa66-43e0-808e-e714b1a1ae01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ex 8.1\n",
    "\n",
    "cnt = collection_rdd.map(lambda x: 1).reduce(add)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dfa0fa-3427-4cd5-84ad-62ea22d10dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ex 8.2\n",
    "a_rdd = sc.parallelize([0, 1, None, [], 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bb6ae-9807-48c4-81f8-030b37ff8fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_rdd.filter(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23334eb-6572-436b-97e2-5a6607ac2107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ex 8.3\n",
    "from typing import Optional\n",
    "\n",
    "def temp_to_temp(value: float, _from: str, to: str) -> Optional[float]:\n",
    "    _from = str.upper(_from)\n",
    "    to = str.upper(to)\n",
    "    assert _from in (\"C\", \"F\", \"K\", \"R\")\n",
    "    assert to in (\"C\", \"F\", \"K\", \"R\")\n",
    "    assert _from != to\n",
    "    \n",
    "    from_to = {\n",
    "        (\"C\", \"F\"): lambda: value * (9 / 5) + 32,\n",
    "        (\"F\", \"C\"): lambda: (value - 32) * (5 / 9),\n",
    "        (\"C\", \"K\"): lambda: value + 273.15,\n",
    "        (\"K\", \"C\"): lambda: value - 273.15,\n",
    "        (\"C\", \"R\"): lambda: value * (9 / 5) + 491.67,\n",
    "        (\"R\", \"C\"): lambda: (value - 491.67) * (5 / 9),\n",
    "        (\"F\", \"K\"): lambda: (value - 32) * (5 / 9) + 273.15,\n",
    "        (\"K\", \"F\"): lambda: (value - 273.15) * (9 / 5) + 32,\n",
    "        (\"F\", \"R\"): lambda: value + 459.67,\n",
    "        (\"R\", \"F\"): lambda: value - 459.67,\n",
    "        (\"K\", \"R\"): lambda: value * (9 / 5),\n",
    "        (\"R\", \"K\"): lambda: value * (5 / 9)\n",
    "    }\n",
    "    \n",
    "    convert = from_to[(_from, to)]\n",
    "    return convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9f930-e89d-416a-bd80-0b3ff86c7d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ex 8.4\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "@F.udf(T.LongType())\n",
    "def naive_udf(t: int) -> Optional[float]:\n",
    "    if not isinstance(t, int):\n",
    "        return None\n",
    "    return t * 3.14159\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568f651-0cb8-426c-890f-dda0b9980b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ex 8.5\n",
    "\n",
    "from fractions import Fraction\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# use existing spark session\n",
    "# spark = SparkSession.builder.appName(\"Ch08 - ex8.5\").getOrCreate()\n",
    "\n",
    "\n",
    "fractions = [[x, y] for x in range(100) for y in range(1, 100)]\n",
    "frac_df = spark.createDataFrame(fractions, [\"numenator\", \"denominator\"])\n",
    "\n",
    "\n",
    "frac_df = frac_df.select(\n",
    "    F.array(F.col(\"numenator\"), F.col(\"denominator\")).alias(\"fraction\")\n",
    ")\n",
    "\n",
    "frac_df.show(5, False)\n",
    "\n",
    "Frac = Tuple[int, int]  # type synonym\n",
    "\n",
    "\n",
    "def py_reduce_fraction(frac: Frac) -> Optional[Frac]:\n",
    "    \"\"\"Reduce a fraction represented as a 2-tuple of integers.\"\"\"\n",
    "    num, denom = frac\n",
    "\n",
    "    if denom:\n",
    "        answer = Fraction(num, denom)\n",
    "        return answer.numerator, answer.denominator\n",
    "    return None\n",
    "\n",
    "\n",
    "assert py_reduce_fraction((3, 6)) == (1, 2)\n",
    "assert py_reduce_fraction((1, 0)) is None\n",
    "\n",
    "\n",
    "def py_fraction_to_float(frac: Frac) -> Optional[float]:\n",
    "    \"\"\"Transform a fraction represented as a 2-tuple of integers into a float.\"\"\"\n",
    "    num, denom = frac\n",
    "\n",
    "    if denom:\n",
    "        return num / denom\n",
    "    return None\n",
    "\n",
    "\n",
    "assert py_fraction_to_float((2, 8)) == 0.25\n",
    "assert py_fraction_to_float((1, 0)) is None\n",
    "\n",
    "\n",
    "SparkFrac = T.ArrayType(T.LongType())\n",
    "\n",
    "reduce_fraction = F.udf(py_reduce_fraction, SparkFrac)\n",
    "\n",
    "frac_df = frac_df.withColumn(\"reduced_fraction\", reduce_fraction(F.col(\"fraction\")))\n",
    "\n",
    "frac_df.show(5, False)\n",
    "\n",
    "\n",
    "@F.udf(T.DoubleType())\n",
    "def fraction_to_float(frac: Frac) -> Optional[float]:\n",
    "    \"\"\"Transform a fraction represented as a 2-tuple of integers as a float.\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        return num / denom\n",
    "    return None\n",
    "\n",
    "\n",
    "frac_df = frac_df.withColumn(\n",
    "    \"fraction_float\", fraction_to_float(F.col(\"reduced_fraction\"))\n",
    ")\n",
    "\n",
    "frac_df.select(\"reduced_fraction\", \"fraction_float\").distinct().show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff588ad-39b2-4839-8298-7170a36945b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ex 8.5\n",
    "\n",
    "@F.udf(SparkFrac) # careful to use the correct type\n",
    "def add_two_fractions(first: Frac, second: Frac) -> Optional[Frac]:\n",
    "    \"\"\"Add two fractions represented as a 2-tuple of integers as a float.\"\"\"\n",
    "    f_num, f_denom = first\n",
    "    s_num, s_denom = second\n",
    "    \n",
    "    if f_denom and s_denom:\n",
    "        \n",
    "        result = Fraction(f_num, f_denom) + Fraction(s_num, s_denom)\n",
    "        return result.numerator, result.denominator\n",
    "        \n",
    "    return None\n",
    "\n",
    "frac_df.printSchema()\n",
    "\n",
    "frac_df = frac_df.withColumn(\n",
    "    \"fraction_doubled\", add_two_fractions(\"reduced_fraction\", \"reduced_fraction\")\n",
    ")\n",
    "\n",
    "frac_df.select(\"fraction\", \"reduced_fraction\", \"fraction_doubled\").distinct().show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbde800-39aa-409e-a62b-58edfe8c38a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ex 8.6\n",
    "\n",
    "@F.udf(SparkFrac)\n",
    "def py_reduce_fraction(frac: Frac) -> Optional[Frac]:\n",
    "    \"\"\"Reduce a fraction represented as a 2-tuple of integers.\"\"\"\n",
    "    num, denom = frac\n",
    "    \n",
    "    MIN_VALUE = -pow(2, 63)\n",
    "    MAX_VALUE = pow(2, 63) - 1\n",
    "\n",
    "    if not denom:\n",
    "        return None\n",
    "    \n",
    "    left, right = Fraction(num, denom).as_integer_ratio()\n",
    "    \n",
    "    if (num < MIN_VALUE or num > MAX_VALUE) or (denom < MIN_VALUE or denom > MAX_VALUE):\n",
    "        return None\n",
    "        \n",
    "    return left, right\n",
    "\n",
    "\n",
    "# No, it doesn't change the type annotation. The type can stay the same as we merely limited the range for the fraction\n",
    "frac_df = frac_df.withColumn(\n",
    "    \"reduced_fraction_2\", py_reduce_fraction(\"fraction\")\n",
    ")\n",
    "\n",
    "frac_df.select(\"fraction\", \"reduced_fraction\", \"reduced_fraction_2\").distinct().show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea1b85-1367-404a-9779-43d8567ee883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
